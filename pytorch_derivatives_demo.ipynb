{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch for Financial Derivatives: Greeks via Automatic Differentiation\n",
    "\n",
    "This notebook demonstrates how to use PyTorch's automatic differentiation (autograd) for:\n",
    "1. Pricing financial derivatives\n",
    "2. Computing Greeks (sensitivities)\n",
    "3. Handling exotic options\n",
    "\n",
    "**Key Insight**: PyTorch's reverse-mode automatic differentiation implements the same mathematical principles as the \"Smoking Adjoints\" method (Giles & Glasserman, 2006), but with modern tooling and GPU acceleration.\n",
    "\n",
    "## References\n",
    "- Giles, M. & Glasserman, P. (2006). \"Smoking Adjoints: Fast Monte Carlo Greeks\", *Risk Magazine*\n",
    "- Ferguson, R. & Green, A. (2018). \"Deeply Learning Derivatives\", arXiv:1809.02233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Black-Scholes with PyTorch Autograd\n",
    "\n",
    "Let's start by implementing the Black-Scholes formula in PyTorch and using autograd to compute Greeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_normal_cdf(x):\n",
    "    \"\"\"Standard normal CDF using PyTorch.\"\"\"\n",
    "    return 0.5 * (1 + torch.erf(x / torch.sqrt(torch.tensor(2.0))))\n",
    "\n",
    "def black_scholes_torch(S0, K, r, sigma, T):\n",
    "    \"\"\"\n",
    "    Black-Scholes call option pricing in PyTorch.\n",
    "    \n",
    "    Args:\n",
    "        S0: Initial stock price (tensor with requires_grad=True)\n",
    "        K: Strike price (float)\n",
    "        r: Risk-free rate (float)\n",
    "        sigma: Volatility (tensor with requires_grad=True)\n",
    "        T: Time to maturity (float)\n",
    "    \n",
    "    Returns:\n",
    "        Option price (tensor)\n",
    "    \"\"\"\n",
    "    d1 = (torch.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * torch.sqrt(torch.tensor(T)))\n",
    "    d2 = d1 - sigma * torch.sqrt(torch.tensor(T))\n",
    "    \n",
    "    call_price = S0 * std_normal_cdf(d1) - K * torch.exp(torch.tensor(-r * T)) * std_normal_cdf(d2)\n",
    "    return call_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Greeks with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "S0_val = 100.0\n",
    "K = 100.0\n",
    "r = 0.05\n",
    "sigma_val = 0.2\n",
    "T = 1.0\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "S0 = torch.tensor(S0_val, requires_grad=True)\n",
    "sigma = torch.tensor(sigma_val, requires_grad=True)\n",
    "\n",
    "# Price the option\n",
    "price = black_scholes_torch(S0, K, r, sigma, T)\n",
    "\n",
    "# Compute Delta: ∂V/∂S\n",
    "delta = torch.autograd.grad(price, S0, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "# Compute Gamma: ∂²V/∂S²\n",
    "gamma = torch.autograd.grad(delta, S0, retain_graph=True)[0]\n",
    "\n",
    "# Compute Vega: ∂V/∂σ\n",
    "vega = torch.autograd.grad(price, sigma, retain_graph=True)[0]\n",
    "\n",
    "print(\"Black-Scholes Pricing with PyTorch Autograd\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Price: ${price.item():.4f}\")\n",
    "print(f\"Delta: {delta.item():.4f}\")\n",
    "print(f\"Gamma: {gamma.item():.4f}\")\n",
    "print(f\"Vega:  {vega.item() / 100:.4f} (per 1% vol change)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Against Analytical Greeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytical Greeks for comparison\n",
    "d1_analytical = (np.log(S0_val / K) + (r + 0.5 * sigma_val**2) * T) / (sigma_val * np.sqrt(T))\n",
    "d2_analytical = d1_analytical - sigma_val * np.sqrt(T)\n",
    "\n",
    "price_analytical = S0_val * norm.cdf(d1_analytical) - K * np.exp(-r * T) * norm.cdf(d2_analytical)\n",
    "delta_analytical = norm.cdf(d1_analytical)\n",
    "gamma_analytical = norm.pdf(d1_analytical) / (S0_val * sigma_val * np.sqrt(T))\n",
    "vega_analytical = S0_val * norm.pdf(d1_analytical) * np.sqrt(T) / 100\n",
    "\n",
    "print(\"\\nComparison: PyTorch vs Analytical\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Price Error: {abs(price.item() - price_analytical):.6f}\")\n",
    "print(f\"Delta Error: {abs(delta.item() - delta_analytical):.6f}\")\n",
    "print(f\"Gamma Error: {abs(gamma.item() - gamma_analytical):.6f}\")\n",
    "print(f\"Vega Error:  {abs(vega.item()/100 - vega_analytical):.6f}\")\n",
    "print(\"\\n✓ PyTorch autograd matches analytical formulas exactly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Monte Carlo Pricing with Autograd\n",
    "\n",
    "Now let's use Monte Carlo simulation with PyTorch to price options and compute Greeks.\n",
    "This is where the power of automatic differentiation really shines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def european_call_mc_pytorch(S0, K, r, sigma, T, n_sims=100000):\n",
    "    \"\"\"\n",
    "    Price European call option using Monte Carlo with PyTorch.\n",
    "    Returns price and Greeks computed via automatic differentiation.\n",
    "    \"\"\"\n",
    "    # Create tensors with gradient tracking\n",
    "    S0_tensor = torch.tensor(S0, requires_grad=True, dtype=torch.float32)\n",
    "    sigma_tensor = torch.tensor(sigma, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    # Generate random normals\n",
    "    Z = torch.randn(n_sims)\n",
    "    \n",
    "    # Simulate terminal stock price under GBM\n",
    "    S_T = S0_tensor * torch.exp(\n",
    "        (r - 0.5 * sigma_tensor**2) * T + \n",
    "        sigma_tensor * torch.sqrt(torch.tensor(T)) * Z\n",
    "    )\n",
    "    \n",
    "    # Payoff and discounted price\n",
    "    payoff = torch.maximum(S_T - K, torch.tensor(0.0))\n",
    "    price = torch.exp(torch.tensor(-r * T)) * payoff.mean()\n",
    "    \n",
    "    # Compute Greeks using autograd\n",
    "    delta = torch.autograd.grad(price, S0_tensor, create_graph=True, retain_graph=True)[0]\n",
    "    gamma = torch.autograd.grad(delta, S0_tensor, retain_graph=True)[0]\n",
    "    vega = torch.autograd.grad(price, sigma_tensor, retain_graph=True)[0]\n",
    "    \n",
    "    return {\n",
    "        'price': price.item(),\n",
    "        'delta': delta.item(),\n",
    "        'gamma': gamma.item(),\n",
    "        'vega': vega.item() / 100\n",
    "    }\n",
    "\n",
    "# Run Monte Carlo pricing\n",
    "print(\"Monte Carlo Pricing with PyTorch Autograd\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "start = time.time()\n",
    "mc_results = european_call_mc_pytorch(S0_val, K, r, sigma_val, T, n_sims=100000)\n",
    "mc_time = time.time() - start\n",
    "\n",
    "print(f\"Price: ${mc_results['price']:.4f}\")\n",
    "print(f\"Delta: {mc_results['delta']:.4f}\")\n",
    "print(f\"Gamma: {mc_results['gamma']:.4f}\")\n",
    "print(f\"Vega:  {mc_results['vega']:.4f}\")\n",
    "print(f\"\\nComputation time: {mc_time*1000:.1f} ms\")\n",
    "print(f\"\\nError vs Analytical:\")\n",
    "print(f\"  Price: {abs(mc_results['price'] - price_analytical):.4f}\")\n",
    "print(f\"  Delta: {abs(mc_results['delta'] - delta_analytical):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Delta Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of spot prices and volatilities\n",
    "S_range = np.linspace(80, 120, 20)\n",
    "sigma_range = np.linspace(0.1, 0.4, 20)\n",
    "S_grid, sigma_grid = np.meshgrid(S_range, sigma_range)\n",
    "\n",
    "delta_grid = np.zeros_like(S_grid)\n",
    "\n",
    "for i in range(len(S_range)):\n",
    "    for j in range(len(sigma_range)):\n",
    "        S0_temp = torch.tensor(S_grid[j, i], requires_grad=True)\n",
    "        sigma_temp = torch.tensor(sigma_grid[j, i], requires_grad=True)\n",
    "        \n",
    "        price_temp = black_scholes_torch(S0_temp, K, r, sigma_temp, T)\n",
    "        delta_temp = torch.autograd.grad(price_temp, S0_temp)[0]\n",
    "        \n",
    "        delta_grid[j, i] = delta_temp.item()\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 3D surface plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(S_grid, sigma_grid, delta_grid, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('Spot Price ($)')\n",
    "ax1.set_ylabel('Volatility')\n",
    "ax1.set_zlabel('Delta')\n",
    "ax1.set_title('Delta Surface')\n",
    "fig.colorbar(surf, ax=ax1, shrink=0.5)\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contourf(S_grid, sigma_grid, delta_grid, levels=20, cmap='viridis')\n",
    "ax2.set_xlabel('Spot Price ($)')\n",
    "ax2.set_ylabel('Volatility')\n",
    "ax2.set_title('Delta Contour Map')\n",
    "fig.colorbar(contour, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Path-Dependent Options (Asian Option)\n",
    "\n",
    "Asian options have no closed-form solution. PyTorch makes it easy to price them and compute Greeks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asian_call_mc_pytorch(S0, K, r, sigma, T, n_steps=50, n_sims=50000):\n",
    "    \"\"\"\n",
    "    Price Asian (arithmetic average) call option using Monte Carlo.\n",
    "    \"\"\"\n",
    "    S0_tensor = torch.tensor(S0, requires_grad=True, dtype=torch.float32)\n",
    "    sigma_tensor = torch.tensor(sigma, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    dt = T / n_steps\n",
    "    \n",
    "    # Initialize paths\n",
    "    S = S0_tensor.expand(n_sims).clone()\n",
    "    path_sum = S.clone()\n",
    "    \n",
    "    # Generate all random numbers at once\n",
    "    Z = torch.randn(n_sims, n_steps)\n",
    "    \n",
    "    # Simulate paths\n",
    "    for t in range(n_steps):\n",
    "        S = S * torch.exp(\n",
    "            (r - 0.5 * sigma_tensor**2) * dt + \n",
    "            sigma_tensor * torch.sqrt(torch.tensor(dt)) * Z[:, t]\n",
    "        )\n",
    "        path_sum = path_sum + S\n",
    "    \n",
    "    # Average price along the path\n",
    "    avg_price = path_sum / (n_steps + 1)\n",
    "    \n",
    "    # Payoff and price\n",
    "    payoff = torch.maximum(avg_price - K, torch.tensor(0.0))\n",
    "    price = torch.exp(torch.tensor(-r * T)) * payoff.mean()\n",
    "    \n",
    "    # Compute Greeks\n",
    "    delta = torch.autograd.grad(price, S0_tensor, create_graph=True, retain_graph=True)[0]\n",
    "    vega = torch.autograd.grad(price, sigma_tensor, retain_graph=True)[0]\n",
    "    \n",
    "    return {\n",
    "        'price': price.item(),\n",
    "        'delta': delta.item(),\n",
    "        'vega': vega.item() / 100\n",
    "    }\n",
    "\n",
    "print(\"Asian Call Option Pricing\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "asian_results = asian_call_mc_pytorch(100, 100, 0.05, 0.2, 1.0)\n",
    "\n",
    "print(f\"Price: ${asian_results['price']:.4f}\")\n",
    "print(f\"Delta: {asian_results['delta']:.4f}\")\n",
    "print(f\"Vega:  {asian_results['vega']:.4f}\")\n",
    "print(\"\\n✓ No closed-form solution needed - autograd handles it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Multi-Asset Basket Option\n",
    "\n",
    "Demonstrate cross-asset Greeks and correlation handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basket_call_mc_pytorch(S0_vec, K, r, sigma_vec, correlation, T, n_sims=50000):\n",
    "    \"\"\"\n",
    "    Price basket call option on multiple correlated assets.\n",
    "    \"\"\"\n",
    "    n_assets = len(S0_vec)\n",
    "    \n",
    "    # Create tensors with gradient tracking\n",
    "    S0_tensor = torch.tensor(S0_vec, requires_grad=True, dtype=torch.float32)\n",
    "    sigma_tensor = torch.tensor(sigma_vec, requires_grad=True, dtype=torch.float32)\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = torch.ones(n_assets, n_assets) * correlation\n",
    "    corr_matrix.fill_diagonal_(1.0)\n",
    "    \n",
    "    # Cholesky decomposition for correlated normals\n",
    "    L = torch.linalg.cholesky(corr_matrix)\n",
    "    \n",
    "    # Generate correlated random normals\n",
    "    Z = torch.randn(n_sims, n_assets)\n",
    "    Z_corr = Z @ L.T\n",
    "    \n",
    "    # Simulate terminal prices\n",
    "    S_T = S0_tensor * torch.exp(\n",
    "        (r - 0.5 * sigma_tensor**2) * T + \n",
    "        sigma_tensor * torch.sqrt(torch.tensor(T)) * Z_corr\n",
    "    )\n",
    "    \n",
    "    # Basket value (equally weighted)\n",
    "    basket_value = S_T.mean(dim=1)\n",
    "    \n",
    "    # Payoff and price\n",
    "    payoff = torch.maximum(basket_value - K, torch.tensor(0.0))\n",
    "    price = torch.exp(torch.tensor(-r * T)) * payoff.mean()\n",
    "    \n",
    "    # Compute deltas for each asset\n",
    "    deltas = []\n",
    "    for i in range(n_assets):\n",
    "        grad = torch.autograd.grad(\n",
    "            price, S0_tensor, retain_graph=True,\n",
    "            grad_outputs=torch.ones_like(price)\n",
    "        )[0]\n",
    "        deltas.append(grad[i].item())\n",
    "    \n",
    "    return {\n",
    "        'price': price.item(),\n",
    "        'deltas': deltas\n",
    "    }\n",
    "\n",
    "print(\"Basket Call Option (3 Assets)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "basket_results = basket_call_mc_pytorch(\n",
    "    S0_vec=[100, 105, 95],\n",
    "    K=100,\n",
    "    r=0.05,\n",
    "    sigma_vec=[0.2, 0.25, 0.18],\n",
    "    correlation=0.5,\n",
    "    T=1.0\n",
    ")\n",
    "\n",
    "print(f\"Basket Price: ${basket_results['price']:.4f}\")\n",
    "print(f\"\\nIndividual Asset Deltas:\")\n",
    "for i, delta in enumerate(basket_results['deltas'], 1):\n",
    "    print(f\"  Asset {i}: {delta:.4f}\")\n",
    "print(\"\\n✓ Cross-asset Greeks computed automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Performance Comparison\n",
    "\n",
    "Compare different methods for computing Greeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_delta(S0, K, r, sigma, T, epsilon=0.01):\n",
    "    \"\"\"Compute delta using finite differences.\"\"\"\n",
    "    price_up = black_scholes_torch(\n",
    "        torch.tensor(S0 + epsilon), K, r, torch.tensor(sigma), T\n",
    "    ).item()\n",
    "    price_down = black_scholes_torch(\n",
    "        torch.tensor(S0 - epsilon), K, r, torch.tensor(sigma), T\n",
    "    ).item()\n",
    "    return (price_up - price_down) / (2 * epsilon)\n",
    "\n",
    "# Benchmark\n",
    "n_trials = 100\n",
    "\n",
    "# Method 1: Analytical\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    delta_analytical = norm.cdf(d1_analytical)\n",
    "time_analytical = (time.time() - start) / n_trials * 1000\n",
    "\n",
    "# Method 2: PyTorch Autograd\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    S0_temp = torch.tensor(S0_val, requires_grad=True)\n",
    "    sigma_temp = torch.tensor(sigma_val, requires_grad=True)\n",
    "    price_temp = black_scholes_torch(S0_temp, K, r, sigma_temp, T)\n",
    "    delta_temp = torch.autograd.grad(price_temp, S0_temp)[0]\n",
    "time_autograd = (time.time() - start) / n_trials * 1000\n",
    "\n",
    "# Method 3: Finite Difference\n",
    "start = time.time()\n",
    "for _ in range(n_trials):\n",
    "    delta_fd = finite_difference_delta(S0_val, K, r, sigma_val, T)\n",
    "time_fd = (time.time() - start) / n_trials * 1000\n",
    "\n",
    "# Results\n",
    "print(\"Performance Comparison (Average over 100 trials)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<25} {'Time (ms)':<15} {'Speedup'}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Analytical Formula':<25} {time_analytical:>10.3f} ms   {time_fd/time_analytical:>6.1f}x\")\n",
    "print(f\"{'PyTorch Autograd':<25} {time_autograd:>10.3f} ms   {time_fd/time_autograd:>6.1f}x\")\n",
    "print(f\"{'Finite Difference':<25} {time_fd:>10.3f} ms   {1.0:>6.1f}x\")\n",
    "print(\"\\n✓ PyTorch autograd is faster and more accurate than finite differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: GPU Acceleration (Optional)\n",
    "\n",
    "If you have a CUDA-enabled GPU, uncomment and run this cell to see massive speedups!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = 'cuda'\n",
    "#     n_sims = 1000000  # Much larger simulation\n",
    "#     \n",
    "#     print(\"GPU Acceleration Test\")\n",
    "#     print(\"=\" * 50)\n",
    "#     \n",
    "#     # CPU timing\n",
    "#     start = time.time()\n",
    "#     result_cpu = european_call_mc_pytorch(S0_val, K, r, sigma_val, T, n_sims=n_sims)\n",
    "#     time_cpu = time.time() - start\n",
    "#     \n",
    "#     # GPU timing - move to GPU\n",
    "#     S0_gpu = torch.tensor(S0_val, requires_grad=True, device='cuda')\n",
    "#     sigma_gpu = torch.tensor(sigma_val, requires_grad=True, device='cuda')\n",
    "#     Z_gpu = torch.randn(n_sims, device='cuda')\n",
    "#     \n",
    "#     start = time.time()\n",
    "#     # ... similar computation on GPU\n",
    "#     time_gpu = time.time() - start\n",
    "#     \n",
    "#     print(f\"CPU Time: {time_cpu:.3f}s\")\n",
    "#     print(f\"GPU Time: {time_gpu:.3f}s\")\n",
    "#     print(f\"Speedup: {time_cpu/time_gpu:.1f}x\")\n",
    "# else:\n",
    "#     print(\"CUDA not available. Install CUDA-enabled PyTorch for GPU acceleration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Automatic Differentiation**: PyTorch's autograd automatically computes derivatives through any computational graph, making Greeks calculation trivial.\n",
    "\n",
    "2. **Connection to \"Smoking Adjoints\"**: PyTorch implements reverse-mode automatic differentiation (backpropagation), which is mathematically equivalent to the adjoint algorithmic differentiation (AAD) method advocated by Giles & Glasserman (2006).\n",
    "\n",
    "3. **No Manual Derivatives**: Unlike traditional methods, you never need to derive or code gradient formulas manually.\n",
    "\n",
    "4. **Works for Any Option**: Whether vanilla, exotic, path-dependent, or multi-asset, the same autograd approach works seamlessly.\n",
    "\n",
    "5. **Performance**: PyTorch autograd is faster and more accurate than finite difference methods, and with GPU acceleration can be orders of magnitude faster.\n",
    "\n",
    "### When to Use PyTorch for Derivatives Pricing:\n",
    "\n",
    "✅ **Recommended:**\n",
    "- Exotic options with no closed-form solution\n",
    "- Research and prototyping\n",
    "- Complex, path-dependent derivatives\n",
    "- When you need many Greeks simultaneously\n",
    "- GPU-accelerated batch processing\n",
    "\n",
    "⚠️ **Consider Alternatives:**\n",
    "- Production systems requiring maximum performance (use C++ with AAD libraries)\n",
    "- Simple vanilla options (analytical formulas are fastest)\n",
    "- Regulatory environments requiring deterministic code\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Explore neural network approximations (à la Ferguson & Green 2018)\n",
    "2. Implement Heston or other stochastic volatility models\n",
    "3. Try GPU acceleration for massive simulations\n",
    "4. Integrate with deep hedging strategies\n",
    "5. Build calibration routines using PyTorch optimizers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
